
#============ ***Ø§Ù„Ù†Ù…ÙˆØ²Ø¬ Ø§Ù„Ø§ÙˆÙ„ ONLINE*** ==============

import google.generativeai as genai

from typing import Dict

class ZIC_Online:
    """Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ†Ù„Ø§ÙŠÙ† (Gemini)"""

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-flash-latest')
        self.chat = self.model.start_chat(history=[])

    def generate_response(self, prompt: str, context: Dict) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†"""
        try:
            enhanced_prompt = f"""
            {context.get('static_memory', '')}

            Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©:
            {context.get('short_memory', '')}

            Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©:
            {context.get('long_memory', '')}

            ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙÙƒÙŠØ±:
            {context.get('thinking_report', '')}

            Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ: {prompt}

            ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ù‡Ù…Ø©:
            1. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ù…Ù† Ø§Ù„Ø°Ø§ÙƒØ±Ø©
            2. ÙƒÙ† Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ ÙˆØµØ±ÙŠØ­Ø§Ù‹
            3. Ø§ÙƒØªØ¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
            4. Ø¥Ø°Ø§ ÙƒÙ†Øª ØºÙŠØ± Ù…ØªØ£ÙƒØ¯ØŒ Ù‚Ù„ Ø°Ù„Ùƒ Ø¨ÙˆØ¶ÙˆØ­
            """

            response = self.chat.send_message(enhanced_prompt)
            return response.text.strip()

        except Exception as e:
            return f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø£ÙˆÙ†Ù„Ø§ÙŠÙ†: {str(e)}"

MY_KEY = "AIzaSyDlUDWdTuRM9dbcBYeXi6AgnfmocAqkxY0"
bot_online = ZIC_Online(MY_KEY)
print("ZIC_Online instance re-initialized with chat memory.")

from datetime import datetime

memory_system = MemorySystem()

start_time = datetime.now()

while True:
  user_input = input("  :   Ø§ÙŠ Ø³ÙˆØ§Ùƒ ÙŠ Ù‚Ù„Ø¨ÙŠ ")
  if "Ø®Ø±ÙˆØ¬" in user_input:
    end_time = datetime.now()
    duration = end_time - start_time
    print(f"ØªÙˆÙ‚ÙØª Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©. Ø¥Ù„Ù‰ Ø§Ù„Ù„Ù‚Ø§Ø¡! Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙƒØ§Ù†Øª: {duration}")
    break
  else :
    print("ğŸŒ Ø¬Ø§Ø±ÙŠ Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ (Online)...")
    context = {
        'static_memory': memory_system.static_memory,
        'short_memory': memory_system.short_memory.get_formatted(),
        'long_memory': memory_system.long_memory.get_formatted(),
        'thinking_report': 'No thinking report available yet.' # Placeholder
    }
    answer = bot_online.generate_response(prompt=user_input, context=context)
    # Assuming speak() will be defined or replaced later
    # For now, print the answer
    speak(answer)
    print(answer)

#========================================================================================================= Ø¨ÙŠØ¦Ù‡ Ø¹Ù…Ù„ Ø§Ù„Ù…ÙˆØ¯ Ø§Ù„ offline ==================================================================================================================

import subprocess
import sys
import os
import time

# --- Comprehensive Ollama Setup ---
print("âš™ï¸ Starting comprehensive Ollama setup...")

def install_ollama_binary():
    ollama_path = '/usr/local/bin/ollama'
    install_script_path = '/tmp/install_ollama.sh'

    # Install zstd if not present, as required by Ollama install.sh
    print("â¡ï¸ Checking for zstd and installing if necessary...")
    try:
        subprocess.run('dpkg -s zstd', shell=True, check=True, capture_output=True)
        print("âœ… zstd is already installed.")
    except subprocess.CalledProcessError:
        print("ğŸ“¥ zstd not found. Installing zstd...")
        try:
            subprocess.run('sudo apt-get update && sudo apt-get install -y zstd', shell=True, check=True)
            print("âœ… zstd installed successfully.")
        except subprocess.CalledProcessError as e:
            print(f"âŒ Failed to install zstd: {e.stderr}")
            sys.exit(1)

    print("ğŸ“¥ Downloading Ollama install.sh script...")
    try:
        # Download the install script
        subprocess.run(f'curl -fsSL https://ollama.com/install.sh -o {install_script_path}', shell=True, check=True)
        print(f"âœ… Install script downloaded to {install_script_path}.")

        # Make it executable
        subprocess.run(f'chmod +x {install_script_path}', shell=True, check=True)
        print(f"âœ… Install script made executable.")

        print("â¡ï¸ Running Ollama install.sh script...")
        # Execute the install script with sudo to ensure proper permissions
        result = subprocess.run(f'sudo {install_script_path}', shell=True, capture_output=True, text=True, check=False) # Do not check=True yet, we want to see output

        if result.returncode != 0:
            print(f"âŒ Ollama install.sh script failed with exit status {result.returncode}.")
            if result.stdout:
                print("Script stdout:")
                print(result.stdout)
            if result.stderr:
                print("Script stderr:")
                print(result.stderr)
            sys.exit(1)
        else:
            print("âœ… Ollama installed successfully via install.sh.")
            if result.stdout:
                print("Script stdout:")
                print(result.stdout)

        # Verify the installation
        file_type = subprocess.run(f'file {ollama_path}', shell=True, capture_output=True, text=True, check=True).stdout.strip()
        print(f"File type of ollama: {file_type}")
        version_output = subprocess.run(f'{ollama_path} --version', shell=True, capture_output=True, text=True, check=True).stdout.strip()
        print(f"âœ… Ollama verified: {version_output}")

    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed to install Ollama: {e}")
        if e.stderr:
            print(f"Stderr: {e.stderr}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ An unexpected error occurred during Ollama installation: {e}")
        sys.exit(1)

def start_ollama_server():
    ollama_path = '/usr/local/bin/ollama'
    print("Starting Ollama server...")
    try:
        # Check if Ollama is already running
        subprocess.check_output(['pgrep', 'ollama']).decode().strip()
        print("âœ… Ollama server is already running.")
        return True
    except subprocess.CalledProcessError:
        pass # Ollama is not running, proceed to start it

    try:
        command = f"nohup {ollama_path} serve > ollama_server.log 2>&1 &"
        subprocess.run(command, shell=True, check=True)
        print("â¡ï¸ Ollama server started in background. Waiting for it to become ready...")

        # Implement a robust check for server readiness
        max_retries = 10
        for i in range(max_retries):
            try:
                # Try to list models, which requires a running server
                subprocess.run(f'{ollama_path} list', shell=True, check=True, capture_output=True, timeout=5)
                print("âœ… Ollama server is ready.")
                return True
            except (subprocess.CalledProcessError, subprocess.TimeoutExpired):
                print(f"Waiting for Ollama server (attempt {i+1}/{max_retries})...")
                time.sleep(5) # Wait 5 seconds before retrying

        print("âŒ Ollama server did not become ready within the timeout.")
        return False
    except Exception as e:
        print(f"âŒ Failed to start Ollama server: {e}")
        return False

def pull_ollama_model(model_name):
    ollama_path = '/usr/local/bin/ollama'
    print(f"Pulling {model_name} model (this may take a while)...")
    try:
        # Ensure ollama is found by using its full path
        pull_command = f"{ollama_path} pull {model_name}"
        process = subprocess.run(pull_command, shell=True, check=True, capture_output=True, text=True)
        print(process.stdout)
        print(f"âœ… {model_name} model pulled successfully.")
    except subprocess.CalledProcessError as e:
        print(f"âŒ Failed to pull {model_name} model: {e.stderr}")
        sys.exit(1)

# Execute setup steps
install_ollama_binary()

# Ensure the model name matches what ZICBrain will use
OLLAMA_MODEL_NAME = "llama3"

if start_ollama_server():
    pull_ollama_model(OLLAMA_MODEL_NAME)
else:
    print("Cannot proceed without a running Ollama server. Please check installation and try again.")
    sys.exit(1)

speak("ØªÙ… ")

#=========================================OFFLINE MODEL===================================

!pip install ollama
import ollama
import sys
import time
from datetime import datetime

class ZICBrain:
    """Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù„Ù€ ZIC"""

    def __init__(self):
        """ØªØ¬Ù‡ÙŠØ² ZIC Ù„Ù„Ø¹Ù…Ù„"""
        self.model_name = "llama3" # Ø§Ø³Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
        self.conversation_history = []  #1 Ù„Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙÙŠ Ø§Ù„Ù€ session Ø§Ù„ÙˆØ§Ø­Ø¯Ø©

        # System Prompt - Ø´Ø®ØµÙŠØ© ZIC
        self.system_prompt = """Ø£Ù†Øª ZICØŒ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ø´Ø®ØµÙŠ Ù„Ø£Ø­Ù…Ø¯.

Ø§Ù„ØµÙØ§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©:
- Ø§Ù†ØªØ§ Ù… Ø¨ØªØªÙƒÙ„Ù… Ø¨Ø·Ø±ÙŠÙ‚Ù‡ Ø±Ø³Ù…ÙŠÙ‡ Ø¨ØªØ³ØªØ¹Ù…Ù„ Ø§Ø®ÙˆÙŠØ§ Ø§Ùˆ Ù…Ø¹Ù„Ù… Ø¨ØªØªØ¹Ø§Ù…Ù„ ÙƒØ§Ù†Ùƒ Ø¹Ø§Ø±ÙÙˆ Ùˆ ÙˆØ§ ØµØ­Ø¨Ùƒ
- ØªØªÙƒÙ„Ù… Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ (Ù…Ø´ Ø¹Ø§Ù…ÙŠØ© Ø®Ø§Ù„ØµØŒ Ù„ÙƒÙ† ÙˆØ§Ø¶Ø­Ø©)
- Ø°ÙƒÙŠØŒ ÙØ¶ÙˆÙ„ÙŠØŒ ÙˆÙ…ØªØ¹Ù„Ù… Ø¯Ø§Ø¦Ù…Ø§Ù‹
- ØµØ±ÙŠØ­: Ù„Ùˆ Ù…Ø´ Ù…ØªØ£ÙƒØ¯ Ù…Ù† Ù…Ø¹Ù„ÙˆÙ…Ø©ØŒ ØªÙ‚ÙˆÙ„ "Ù…Ø´ Ù…Ø´ Ù…ØªØ£ÙƒØ¯" Ø¨Ø¯Ù„ Ù…Ø§ ØªØ®ØªØ±Ø¹
- ØªÙÙƒØ± Ù‚Ø¨Ù„ Ù…Ø§ ØªØ±Ø¯
- ØªØ³Ø£Ù„ Ø£Ø³Ø¦Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ© Ù„Ùˆ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù…Ø´ ÙˆØ§Ø¶Ø­
- Ø§Ù„ÙŠ ØµÙ†Ø¹Ùƒ Ø§Ø­Ù…Ø¯ Ø§Ù†ØªØ§ Ø·Ø±ÙŠÙ Ùˆ Ø±Ø¯ÙˆØ¯Ùƒ Ø¯Ø§ÙŠÙ…Ø§ Ù„Ø§Ø²Ù… ØªØ¨Ù‚ÙŠ ØµØ­ Ùˆ Ù…ØªØ§ÙƒØ¯ Ù…Ù†Ù‡Ø§
- Ù…Ø­Ø¨ Ùˆ ÙØ¶ÙˆÙ„ÙŠ Ø§ÙŠ Ø³ÙˆØ§Ù„ Ù…Ø´ Ø¹Ø§Ø±ÙÙˆ Ø¨ØªØ³Ø§Ù„ ÙÙŠÙ‡
Ø§Ù„Ù…Ù‡Ù…Ø©: Ù…Ø³Ø§Ø¹Ø¯Ø© Ø£Ø­Ù…Ø¯ ÙÙŠ Ø£ÙŠ Ø´ÙŠØ¡ ÙŠØ­ØªØ§Ø¬Ù‡ Ø¨Ø°ÙƒØ§Ø¡ ÙˆØ§Ø­ØªØ±Ø§ÙÙŠØ©."""



        print("ğŸ¤– ZIC Ø¨ÙŠØªØ¬Ù‡Ø²...")
        # connect_to_model will now return True/False instead of exiting
        # The calling code in H1Y26fnx2ixX will handle the result.

    def connect_to_model(self):
        """Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ llama3"""
        try:
            print("ğŸ”Œ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù€ ZIC Offline ...")

            # Ø§Ø®ØªØ¨Ø§Ø± Ø¨Ø³ÙŠØ·
            response = ollama.chat(
                model=self.model_name,
                messages=[{"role": "user", "content": "test"}],
                options={"num_predict": 5}  # Ø±Ø¯ Ù‚ØµÙŠØ± Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙ‚Ø·
            )

            if response:
                print("âœ… done ÙŠÙ‚Ù„Ø¨ÙŠ\n")
                return True

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„:")
            print(f"   {str(e)}")
            print("\nğŸ’¡ ØªØ£ÙƒØ¯ Ù…Ù†:")
            print("   1. Ollama Ù…ÙØªÙˆØ­ ÙˆÙŠØ¹Ù…Ù„")
            print("   2. Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ llama3 Ù…Ø­Ù…Ù‘Ù„ (ollama pull llama3)") # Updated model name in message
            return False # Return False on failure instead of exiting

    def ask_zic(self, question, use_history=True):
        """
        Ø¥Ø±Ø³Ø§Ù„ Ø³Ø¤Ø§Ù„ Ù„Ù€ ZIC ÙˆØ§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø±Ø¯

        Args:
            question (str): Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ù…Ø·Ø±ÙˆØ­
            use_history (bool): Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø£Ù… Ù„Ø§

        Returns:
            str: Ø±Ø¯ ZIC
        """
        try:
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø±Ø³Ø§Ø¦Ù„
            messages = []

            # System prompt Ø£ÙˆÙ„Ø§Ù‹
            messages.append({
                "role": "system",
                "content": self.system_prompt})
            if use_history and self.conversation_history:
                messages.extend(self.conversation_history[-10:])  # Ø¢Ø®Ø± 10 Ø±Ø³Ø§Ø¦Ù„ (5 Ø£Ø³Ø¦Ù„Ø© ÙˆØ£Ø¬ÙˆØ¨Ø©)

            # Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„Ø­Ø§Ù„ÙŠ
            messages.append({
                "role": "user",
                "content": question
            })

            print("ğŸ’­ ZIC Ø§ØªÙ‚Ù„ Ø¨ÙÙƒØ± ", end="", flush=True)

            # Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
            start_time = time.time()

            response = ollama.chat(
                model=self.model_name,
                messages=messages,
                options={
                    "temperature": 0.7,      # Ù…ØªÙˆØ§Ø²Ù† Ø¨ÙŠÙ† creativity ÙˆØ¯Ù‚Ø©
                    "top_p": 0.9,
                    "num_predict": 500,      # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„Ù„Ø±Ø¯
                }
            )
            print(f"DEBUG: Raw Ollama response object: {response}") # Debug print to see what the model returns

            # Check if 'message' key exists and then 'content'
            if 'message' in response and 'content' in response['message']:
                answer = response['message']['content'].strip()
                print(f"DEBUG: Extracted content: {answer}")
            else:
                answer = "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø±Ø¯ ØµØ§Ù„Ø­ Ù…Ù† Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„."
                print(f"DEBUG: Invalid response structure: {response}")

            elapsed_time = time.time() - start_time
            print(f"\râ±ï¸  ZIC ÙÙƒØ± ÙÙŠ {elapsed_time:.1f} Ø«Ø§Ù†ÙŠØ©")

            # Ø­ÙØ¸ ÙÙŠ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            self.conversation_history.append({
                "role": "user",
                "content": question
            })
            self.conversation_history.append({
                "role": "assistant",
                "content": answer
            })

            return answer

        except KeyboardInterrupt:
            print("\n\nâš ï¸  ØªÙ… Ø¥ÙŠÙ‚Ø§Ù ZIC Ù…Ù† Ø§Ù„ØªÙÙƒÙŠØ±")
            return None

        except Exception as e:
            print(f"\nâŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø¯: {str(e)}")
            return None

    def clear_history(self):
        """Ù…Ø³Ø­ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        self.conversation_history = []
        print("ğŸ—‘ï¸  ØªÙ… Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø©")

    def save_conversation(self, filename=None):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙÙŠ Ù…Ù„Ù Ù†ØµÙŠ"""
        if not self.conversation_history:
            print("âš ï¸  Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ø­ÙØ¸Ù‡Ø§")
            return

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"zic_conversation_{timestamp}.txt"

        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write("=" * 50 + "\n")
                f.write("ZIC Conversation Log\n")
                f.write(f"Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("=" * 50 + "\n\n")

                for msg in self.conversation_history:
                    role = "Ø£Ù†Øª" if msg['role'] == 'user' else "ZIC"
                    f.write(f"{role}: {msg['content']}\n\n")

            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙÙŠ: {filename}")

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©: {str(e)}")

    def run(self):
        """ØªØ´ØºÙŠÙ„ ZIC - Ø§Ù„Ù€ Loop Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ"""
        print("=" * 60)
        print("ğŸ¤– ZIC Ø¬Ø§Ù‡Ø² Ù„Ù„Ø­ÙˆØ§Ø±!")
        print("=" * 60)
        print("\nğŸ’¡ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ù…ØªØ§Ø­Ø©:")
        print("   - Ø§ÙƒØªØ¨ Ø£ÙŠ Ø³Ø¤Ø§Ù„ Ù„Ù„Ø­ÙˆØ§Ø±")
        print("   - 'Ù…Ø³Ø­' â†’ Ù…Ø³Ø­ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø©")
        print("   - 'Ø­ÙØ¸' â†’ Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©")
        print("   - 'Ø®Ø±ÙˆØ¬' â†’ Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬")
        print("\n" + "=" * 60 + "\n")

        while True:
            try:
                # Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„
                user_input = input("Ø£Ù†Øª: ").strip()

                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ø§Ù„Ø®Ø§ØµØ©
                if not user_input:
                    continue

                if user_input.lower() in ['Ø®Ø±ÙˆØ¬', 'exit', 'quit', 'bye']:
                    print("\nğŸ‘‹ ZIC: Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙŠØ§ Ø£Ø­Ù…Ø¯! Ø§Ø³ØªÙ…ØªØ¹Øª Ø¨Ø§Ù„Ø­ÙˆØ§Ø±")

                    # Ø³Ø¤Ø§Ù„ Ø¹Ù† Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
                    if self.conversation_history:
                        save = input("\nğŸ’¾ Ø¹Ø§ÙŠØ² ØªØ­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©ØŸ (Ù†Ø¹Ù…/Ù„Ø§): ").strip().lower()
                        if save in ['Ù†Ø¹Ù…', 'yes', 'y', 'Ù†']:
                            self.save_conversation()

                    break

                if user_input.lower() in ['Ù…Ø³Ø­', 'clear', 'reset']:
                    self.clear_history()
                    continue

                if user_input.lower() in ['Ø­ÙØ¸', 'save']:
                    self.save_conversation()
                    continue

                # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø³Ø¤Ø§Ù„ Ù„Ù€ ZIC
                answer = self.ask_zic(user_input)

                if answer:
                    speak(f"\nZIC: {answer}\n")
                    print("-" * 60 + "\n")

            except KeyboardInterrupt:
                print("\n\nâš ï¸  ØªÙ… Ø¥ÙŠÙ‚Ø§Ù ZIC")
                break

            except Exception as e:
                print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {str(e)}\n")




if __name__ == "__main__":
    MAX_OLLAMA_RETRIES = 3
    ollama_connected = False
    for attempt in range(MAX_OLLAMA_RETRIES):
        print(f"Attempt {attempt + 1}/{MAX_OLLAMA_RETRIES} to connect to Ollama...")
        # Re-run ollama server start and model pull just in case it died
        # We need to make sure start_ollama_server and pull_ollama_model functions are available in the scope
        # For now, assuming they are imported or available. If not, this cell would need full setup.
        # Given this is a Colab notebook, the functions are defined in zfmukFC-0cNl and would be in scope if executed.
        start_ollama_server() # Ensure server is running
        pull_ollama_model(OLLAMA_MODEL_NAME) # Ensure model is loaded

        brain_instance = ZICBrain() # Instantiate ZICBrain
        if brain_instance.connect_to_model(): # Try to connect
            ollama_connected = True
            break
        else:
            print("Ollama connection failed. Retrying in 5 seconds...")
            time.sleep(5)

    if ollama_connected:
        try:
            brain_instance.run()
        except KeyboardInterrupt:
           speak("\n\nğŸ‘‹ ØªÙ… Ø¥ØºÙ„Ø§Ù‚ ZIC")
        except Exception as e:
            speak(f"\nâŒ Ø®Ø·Ø£ ÙØ§Ø¯Ø­: {str(e)}")
    else:
        speak("âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø®Ø¯Ù…Ø© Ollama Ø¨Ø¹Ø¯ Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ollama ÙŠØ¯ÙˆÙŠÙ‹Ø§.")





